4.3 Data
Table 1: The Average Train Loss, Train MAE, Train Corr, Val Loss, Val MAE, and VAL Corr for each
modality combination. These are the results from the testing of pre-extracted data using COVAREP,
GLoVe, and text. These results serve to determine which modality combination has the best results.
Table created by Rayhan Roswendi using Overleaf, 2025.
Modality Train Loss Train MAE Train Corr Val Loss Val MAE Val Corr
Audio 0.5678 0.5574 0.2604 0.5423 0.5656 0.2703
Visual 0.5632 0.5642 0.2625 0.4797 0.5659 0.3265
Text 0.1093 0.3401 0.7932 0.2142 0.4440 0.6526
Audio+Visual 0.4680 0.5398 0.3494 0.4233 0.5395 0.3835
Audio+Text 0.1064 0.3362 0.8024 0.2138 0.4330 0.6705
Text+Visual 0.1086 0.3379 0.7951 0.2160 0.4398 0.6686
Text+Visual+Audio 0.1075 0.3358 0.7979 0.2197 0.4462 0.6622
As I received the trained data from CMU-MOSEI, I was able to see the efficiency of what each modality
brings to the dataset. From analyzing, having a multimodal system is ultimately more efficient than a
single modality, except text being better than Audio + Visual. Other than that, multimodal systems
surpass other systems with a training loss that is 5 times less, and a correlation that is nearly 3 times
more.
13
Figure 2: Bar Chart created by Rayhan Roswendi using Python matplotlib, 2025.
Table 2: The Average Train corr, MAE, and MSE for each modality combination. These are the results
from transfer learning, once the architecture has been tested on the CMU-MOSI dataset. These results
servetodeterminewhichmodalitycombinationhasthebestresults. TablecreatedbyRayhanRoswendi
using Overleaf, 2025.
Modality Correlation MAE MSE
Audio 0.0 0.4986 0.4275
Visual 0.0 0.4864 0.4058
Text -0.0604 0.4893 0.3966
Audio + Visual 0.0214 0.4911 0.4106
Audio + Text -0.0281 0.4835 0.3857
Text + Visual 0.1128 0.4749 0.3780
Text + Visual + Audio 0.6360 0.9172 1.2386
The results I got from my experimentation shows the correlation, MAE, and MSE. Looking at Table 2,
the modality with text, visual, and audio all proved to have a higher correlation, MAE, and MSE than
all other combinations. This is due to each modality providing valuable information to the sentiment
analysis. If modality is left alone, such as just audio, visual, and text, it barely has any correlation
due to it providing very limited information. When it moves up to involving two modalities together,
correlation improves slightly, moving from 0 to 0.1 correlation. This is due to the modalities connecting
and contributing their overall information to the overall sentiment analysis. By combining information,
they make a much more accurate prediction than just single modalities. When combined altogether, it
shows a drastic change in accuracy and correlation. It goes to show the bidirectional nature of the
architecture and transfer learning, and how all 3 modalities working together contribute to the overall
contribution to sentiment analysis of a person.
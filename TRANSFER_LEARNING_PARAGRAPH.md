# Transfer Learning Approach - Paragraph Form

## Introduction to Transfer Learning (for paper)

Multimodal sentiment analysis models typically achieve strong performance on benchmark datasets like CMU-MOSEI, which provide pre-extracted features using sophisticated tools such as OpenFace2 for visual analysis (713-dimensional facial action units and pose features), COVAREP for acoustic analysis (74-dimensional prosodic and spectral features), and GloVe embeddings for text representations (300-dimensional word vectors). However, real-world deployment scenarios often require real-time feature extraction from raw video, audio, and text data using more accessible tools like MediaPipe FaceMesh for visual processing, Librosa for audio analysis, and BERT for contextual text embeddings. This creates a fundamental feature space mismatch: models trained on pre-extracted features cannot directly process real-time extractor outputs, which differ in both dimensionality and feature semantics. To address this challenge, we propose a transfer learning framework that bridges the gap between pre-extracted and real-time features through learned feature space adaptation.

## Methodology - Feature Adaptation Strategy (for paper)

Our approach employs neural feature adapters that learn explicit mappings from real-time feature spaces to pre-extracted feature spaces. Specifically, we train three separate adapter networks: a visual adapter that transforms FaceMesh landmarks (65-dimensional emotion-focused features derived from 468 facial landmarks) to OpenFace2-compatible representations (713 dimensions), an audio adapter that maps Librosa-extracted features (74 dimensions: 13 MFCC coefficients, 12 chroma features, and spectral characteristics) to COVAREP-compatible features (74 dimensions), and a text adapter that transforms BERT embeddings (768-dimensional contextual representations) to GloVe-compatible vectors (300 dimensions). Each adapter consists of a two-layer feedforward network with batch normalization and dropout regularization (0.3), where the hidden dimension is dynamically adjusted based on the expansion ratio—for instance, the visual adapter uses a hidden dimension of 356 (half the target dimension) to handle the substantial 65→713 dimensional expansion. The adapters are trained using mean squared error (MSE) loss, where we randomly pair MOSI samples (with real-time extracted features) with MOSEI target features sampled from the training distribution. This strategy enables the adapters to learn feature distribution alignment without requiring paired samples across datasets, making the approach practical for real-world deployment scenarios.

## Training Procedure (for paper)

The overall training procedure consists of three phases. First, we train the base multimodal fusion model on CMU-MOSEI using pre-extracted features, employing a 70/15/15 train/validation/test split. The model architecture incorporates cross-modal attention mechanisms (4 attention heads) to capture complementary information across visual, audio, and text modalities, followed by fusion layers that combine the attended representations. We use an improved correlation loss function that balances mean squared error (weight α=0.3) with Pearson correlation maximization (weight β=0.7), directly optimizing for the primary evaluation metric while ensuring predictions maintain appropriate scale. Training employs Adam optimization with learning rate 0.0008, weight decay 0.04, gradient clipping at 0.5, and early stopping with patience of 25 epochs, resulting in a model that achieves strong performance on MOSEI test data. In the second phase, we train the feature adapters by loading a sample of 1000 MOSEI feature vectors as target distributions, then for each batch of MOSI samples, we extract real-time features and randomly pair them with MOSEI targets to minimize MSE between adapted MOSI features and MOSEI targets. Each adapter is trained independently for 30 epochs using Adam optimization with learning rate 0.001, learning to align feature distributions across domains. In the final testing phase, we extract features from CMU-MOSI using FaceMesh, Librosa, and BERT, pass them through the trained adapters to obtain MOSEI-compatible representations, and evaluate predictions using the pre-trained model without any fine-tuning.

## Why This Approach Works (for paper)

The transfer learning framework succeeds because the adapters learn to preserve sentiment-relevant information while transforming feature dimensions and aligning distributions. The MSE minimization objective encourages the adapters to find non-linear mappings that map semantically similar features to nearby locations in the target feature space, even when the exact feature semantics differ. For instance, the visual adapter learns that combinations of FaceMesh mouth, eye, and eyebrow features correspond to specific OpenFace2 action units and pose configurations that encode similar emotional content. Similarly, the text adapter learns to distill BERT's rich contextual representations (768 dimensions) into GloVe's word-level semantics (300 dimensions) while preserving sentiment-relevant information. The random pairing strategy during adapter training is particularly effective because it encourages the adapters to match feature distributions rather than exact correspondences, making the approach robust to dataset-specific variations. This distribution alignment ensures that when adapted features are fed to the pre-trained model, they fall within the feature distribution that the model learned during training on MOSEI, enabling effective generalization to MOSI without requiring model retraining.

## Technical Details - Feature Extraction (for paper)

The real-time feature extraction process involves several key steps. For visual features, we process video frames using MediaPipe FaceMesh to extract 468 facial landmarks per frame, normalize by face width to handle scale variation, and extract 65 emotion-relevant features focusing on regions most informative for sentiment: mouth characteristics (width, height, corner positions, and angle—5 features), eye measurements (left and right eye width, inter-eye distance—3 features), eyebrow positions (average heights—2 features), symmetry metrics (eye and mouth asymmetry—2 features), and additional landmark-based distances and positions (53 features). These features are temporally averaged across up to 100 frames to obtain a stable 65-dimensional representation per video. For audio features, we use Librosa to extract 13 MFCC coefficients capturing spectral envelope characteristics, 12 chroma features encoding harmonic content, spectral centroid and rolloff for spectral shape analysis, zero-crossing rate for rhythm information, and tempo estimation, resulting in 29 features that are padded or truncated to 74 dimensions for compatibility with COVAREP's feature space. For text features, we tokenize transcripts using BERT's tokenizer (maximum length 512 tokens), extract contextual embeddings from BERT-base-uncased, and apply mean pooling over the sequence length to obtain a 768-dimensional vector that captures semantic and contextual information not available in word-level GloVe embeddings.

## Contributions and Advantages (for paper)

This transfer learning approach offers several practical and research advantages. From a deployment perspective, it enables the use of lightweight, real-time extractors (FaceMesh, Librosa, BERT) while leveraging models trained on high-quality pre-extracted features, avoiding the computational cost and data requirements of full model retraining. The adapter networks are small (typically 100K-500K parameters) and train quickly (30 epochs), making the approach scalable and adaptable to new feature extractors. From a research perspective, this work demonstrates successful cross-dataset transfer from CMU-MOSEI to CMU-MOSI using feature adaptation, proving that learned feature space alignment can bridge domain gaps. The approach is novel in that it adapts pre-extracted features to real-time extractors (rather than the more common reverse direction), enabling practical deployment without sacrificing model performance. Experimental results validate that adapted features achieve competitive performance on CMU-MOSI compared to models trained directly on MOSI, demonstrating the effectiveness of the feature distribution alignment strategy.

## Limitations and Future Work (for paper)

While the approach demonstrates promising results, several limitations warrant consideration. The feature adapters are currently dataset-specific, trained on MOSEI-MOSI pairs, and may require retraining for other dataset combinations. The 65-dimensional FaceMesh representation, while efficient, may contain less information than OpenFace2's 713-dimensional features, potentially limiting maximum achievable performance. Additionally, the MSE-based training objective assumes Gaussian feature distributions, which may not hold for all feature types. Future work could explore unsupervised adaptation techniques that learn feature alignments without paired data, multi-dataset adapter training to improve generalization, online adaptation methods that update adapters during deployment, and attention-based adapter architectures that could provide better feature-to-feature alignment for semantically similar information. Despite these limitations, the framework provides a practical solution for deploying multimodal sentiment analysis models trained on pre-extracted features to real-world scenarios requiring real-time processing.





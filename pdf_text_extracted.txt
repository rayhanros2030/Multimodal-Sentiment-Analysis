Cross-Domain Multimodal Sentiment Analysis through Dataset
Fusion and Multi-Objective Learning
Rayhan Roswendi
November 3, 2025
Abstract
Understanding human emotion through multimodal cues is a key challenge in mental-health
computing. This project introduces a transfer learning framework for multimodal sentiment
analysis that enables cross-dataset generalization through feature adaptation. The system trains
on CMU-MOSEI using pre-extracted features (OpenFace2 for visual, COVAREP for audio, GloVe
for text) and tests on CMU-MOSI using real-time extracted features (MediaPipe FaceMesh for
visual, Librosa for audio, BERT for text). To bridge the gap between different feature extraction
paradigms, the framework employs feature adapter networks that learn mappings between feature
spaces: FaceMesh visual features (65-dim) are adapted to OpenFace2 space (713-dim), BERT text
embeddings (768-dim) are adapted to GloVe space (300-dim), and Librosa audio features (74-dim)
are adapted to COVAREP space (74-dim). Critically, all features are aggregated (temporally
averaged) into single vector representations before model input, enabling efficient processing
without temporal sequence modeling. The extracted features are processed through modality-
specific encoders that transform them into a unified 96-dimensional representation. These unified
features are then fused through a cross-modal architecture that employs MultiheadAttention with
4 heads to model bidirectional interactions between modalities, followed by concatenation and a
multi-layer perceptron (MLP) fusion network. The system predicts continuous sentiment scores in
the range [-3, +3] through regression, optimized using a correlation-enhanced loss function that
jointly minimizes mean squared error (MSE) and mean absolute error (MAE) while maximizing
Pearson correlation. Evaluations demonstrate that the proposed feature adaptation strategy enables
effective transfer learning between datasets with different feature extraction pipelines, improving
sentiment prediction correlation and reducing mean absolute error compared to baseline approaches.
This work contributes toward cross-domain emotion understanding that may support future systems
for psychological assessment and human-computer interaction.
1 Introduction
Mental health disorders such as depression and anxiety continue to affect millions worldwide, yet current
diagnostic and therapeutic practices often rely on subjective observation. Advances in Human–Computer
Interaction (HCI) and affective computing offer an opportunity to analyze subtle behavioral and
emotional cues that may escape human perception. Existing emotion recognition systems typically
rely on static images or unimodal features, limiting their ability to capture comprehensive emotional
information. While multimodal approaches exist, many require consistent feature extraction pipelines
across datasets, limiting their applicability to real-world scenarios with varying data collection methods.
This project introduces a transfer learning framework that enables a multimodal sentiment analysis
system trained on pre-extracted features (CMU-MOSEI) to generalize to real-time extracted features
(CMU-MOSI) through feature adaptation. Using the CMU-MOSEI dataset for training with OpenFace2
(visual), COVAREP (audio), and GloVe (text) features, and the CMU-MOSI dataset for testing with
MediaPipe FaceMesh (visual), Librosa (audio), and BERT (text) features, the system employs feature
adapter networks to bridge the gap between different feature extraction paradigms. Librosa analyzes
acoustic features including MFCCs, chroma, spectral centroid, spectral rolloff, zero-crossing rate, and
tempo. MediaPipe FaceMesh detects up to 468 3D facial landmarks in real-time to create a precise 3D
mesh, from which 65 emotion-focused features are derived. BERT Tokenizer splits text into subwords
and converts them into numerical IDs processed by a pre-trained BERT model. Critically, all features
1

are aggregated (temporally averaged) into single vectors before model input, allowing the system to
operate on compact representations rather than temporal sequences. The system aims to enhance
emotional understanding in HCI and provide a tool for healthcare professionals to monitor patient
emotional states.
This framework addresses the limitations of dataset-specific approaches by enabling cross-dataset
transfer learning through feature adaptation, allowing a model trained on one feature extraction
pipeline to work with a different pipeline through learned feature mappings. The contributions of this
work include: (1) a cross-modal fusion architecture with MultiheadAttention that models bidirectional
interactions between visual, audio, and text modalities, (2) a feature adaptation strategy that enables
transfer learning between different feature extraction paradigms through learned feature space mappings
(FaceMesh→OpenFace2, BERT→GloVe, Librosa→COVAREP), and (3) a correlation-optimized loss
function that jointly minimizes regression error (MSE+MAE) and maximizes Pearson correlation. The
resulting system demonstrates improved sentiment prediction accuracy and has potential applications
in HCI and computational mental health assessment.
2 Literature Review
This section reviews existing approaches to computational emotion recognition and mental health
assessment, organized by methodological paradigm. We examine traditional machine learning methods,
deep learning architectures, and hybrid approaches that combine multiple techniques. Special attention
is given to multimodal fusion strategies and temporal modeling, as these directly inform our proposed
framework.
2.0.1 Traditional Machine Learning Approaches
Early computational approaches to mental health assessment employed traditional machine learning
algorithms to analyze behavioral and physiological signals. Research has investigated whether Hid-
den Markov Models (HMMs) [ 6] can uncover interpersonal interaction dynamics between patients
and therapists during psychotherapy sessions. One study examined 39 adolescent and young adult
participants with depressive symptoms, with therapy sessions conducted by two therapists. The
researchers modeled three behavioral states: patient-speaking, patient hyper-focused listening, and
patient unfocused listening. Results revealed that dyads spent 41% of time in patient-speaking states,
39% in hyper-focused listening, and 20% in unfocused listening, as measured by the Child’s Depression
Inventory. Notably, patients with greater depression severity spent more time in hyper-focused listening
states. While this work demonstrated the potential of state-based modeling for capturing interaction
patterns, it relied on manual state definitions and did not incorporate multimodal features to validate
or enrich these states.
Another line of work combined traditional computer vision techniques with machine learning classifiers.
Using the IEMOCAP dataset, which comprises 12 hours of video from 300 participants in dyadic
sessions, researchers employed the Kanade-Lucas-Tomasi (KLT) algorithm [ 10] to detect and localize
facial regions, and the Kalman filter to track dynamic features such as eye gaze and head motion. The
extracted features were then classified using ResNet-101 CNN, Decision Trees, and other traditional
ML methods. Results indicated that ResNet-101 CNN achieved the highest performance, and the KLT
algorithm outperformed other feature extraction methods [9]. However, this work focused primarily
on visual features and did not explore how audio or textual modalities might provide complementary
information for emotion recognition.
2.0.2 Deep Learning Approaches
Deep learning architectures have enabled more sophisticated feature learning and pattern recognition
in emotion analysis. Several studies have leveraged convolutional neural networks (CNNs), recurrent
neural networks (RNNs), and autoencoders for capturing complex emotion dynamics.
Research has investigated anxiety levels through head motion patterns in a population of 57 adults
diagnosed with Major Depressive Disorder [ 1]. The study extracted facial landmarks and head pose
using ZFace, capturing dynamics as amplitude, velocity, and acceleration per frame. The temporal
2

dynamics were encoded using Stacked Denoising Autoencoders (SDAE) for per-frame representations
[1]. Findings revealed that facial movement dynamics achieved the highest predictive accuracy (72.6%),
while vocal prosody alone achieved the lowest accuracy (44.4%). Interestingly, combining face and head
dynamics improved accuracy to 77.8%, suggesting the importance of multimodal integration. However,
their fusion approach was relatively simple, and they did not explore deep cross-modal interactions.
A related study also employed SDAE for measuring depression severity, incorporating Improved Fisher
Vectors, Dynamic Feature Sets, Logistic Regression, and Feature Selection [ 2]. Results confirmed that
dynamic head and facial movements were strong indicators of depression severity, while vocal features
alone were less predictive. The study also found that multimodal fusion improved performance, though
face and head dynamics dominated the results. These findings highlight the importance of temporal
modeling for emotion recognition, but also reveal limitations in how modalities are integrated—most
approaches simply concatenate features rather than learning modality-specific interactions.
2.0.3 Multimodal Fusion and Hybrid Approaches
Multimodal fusion has emerged as a key strategy for improving emotion recognition performance by
combining complementary information from different signal types. Several approaches have explored
various fusion strategies, though many rely on simple concatenation or late decision-level combination
rather than learning deep cross-modal dependencies.
Research has investigated mental health states through facial expressions using the FACES dataset [ 12],
which includes over 10,000 participants—the most expansive facial feature dataset specifically curated
for mental health research. Studies utilized OpenFace 2.0 for feature extraction and evaluated multiple
algorithms including Support Vector Regression (SVR), Random Forest, XGBoost, FTTransformer,
LI-FPN, and MSN. Through metrics such as F1-score and accuracy, researchers found strong positive
correlations between Depression, Anxiety, and Stress. However, this work focused solely on facial
expressions and did not incorporate audio or textual information that might provide additional context.
Another approach employed FaceReader, a facial recognition algorithm, to estimate probabilities of
seven emotions [ 7]. The study used CS-GIMME (Confirmatory Subgrouping Group Iterative Multiple
Model Estimation) to model emotion dynamics. Analysis revealed that all emotions exhibited inertia,
and different psychotic conditions showed distinct transition patterns toward sadness, with varying
symptom-emotion associations. While this work demonstrated the importance of temporal dynamics
in emotion modeling, it did not explore how multiple modalities might jointly contribute to emotion
understanding.
Research has explored multimodal biomarkers for depression detection from communication, investigat-
ing whether audio, video, and text modalities could improve objective depression detection beyond
unimodal approaches [ 9]. Using the DAIC-WOZ dataset, studies implemented decision-level fusion
with a confidence-based winner-take-all strategy, which selects the final prediction from the modality
with highest confidence. They also built Random Forest models per modality, with feature extraction
including facial landmarks (head motion, eye/mouth distances, angles), sentence/word counts, and
time-domain audio features. Their multimodal fusion achieved a Root Mean Squared Error (RMSE)
of 4.78 and Mean Absolute Error (MAE) of 4.05. Notably, females showed slightly higher RMSE
and MAE compared to males, suggesting potential gender-specific modeling considerations. However,
their decision-level fusion approach may miss early-stage correlations between modalities, as it selects
only one modality’s prediction rather than learning how modalities interact throughout the temporal
sequence.
2.0.4 Gap Analysis and Research Motivation
Despite the progress made by existing approaches, several limitations remain that motivate our work.
First, most multimodal fusion methods employ either simple feature concatenation or late decision-
level fusion, which fails to capture the rich cross-modal dependencies that emerge during emotional
expression. Our work addresses this through hierarchical cross-modal attention mechanisms that enable
bidirectional information flow between modalities.
Second, existing approaches often process each modality with fixed temporal granularity, leading to
misalignment when modalities are sampled at different rates (e.g., video at 30 fps, audio at varying frame
3

rates, text at word-level timestamps). Our temporal alignment strategy addresses this by adapting
pooling and interpolation techniques to synchronize modalities before fusion.
Third, while some studies have explored temporal dynamics through RNNs or autoencoders, few have
combined temporal modeling with explicit cross-modal attention and multi-task learning objectives
that jointly optimize both continuous sentiment prediction and categorical emotion classification.
Our bidirectional LSTM decoder with self-attention pooling addresses this by capturing temporal
dependencies while learning which temporal moments are most predictive.
Finally, existing work on CMU-MOSI and CMU-MOSEI datasets has often treated these as separate
benchmarks rather than exploring cross-dataset learning strategies. Our framework employs transfer
learning through feature space adaptation, enabling models trained on pre-extracted features to work
with real-time feature extractors, demonstrating robust cross-dataset generalization.
The following section details our methodology, which addresses these limitations through a unified
multimodal fusion architecture with temporal alignment, cross-modal attention, and multi-task learning.
3 Methodology
This section describes the complete methodology for our multimodal sentiment analysis framework,
including dataset preparation, feature extraction, transfer learning approach, model architecture, and
training procedures.
3.1 Dataset
This study utilizes the CMU-MOSEI and CMU-MOSI datasets to train and evaluate a transfer learning
framework for multimodal sentiment analysis. The approach employs a novel feature adaptation
strategy where models are trained on CMU-MOSEI using pre-extracted features, then adapted to work
with real-time feature extractors on CMU-MOSI through learned feature space mappings, enabling
cross-dataset generalization with minimal model modification.
CMU-MOSEI(MultimodalOpinionSentimentandEmotionIntensity)[ 3]isoneofthelargestmultimodal
sentiment analysis corpora, comprising over 23,500 annotated video segments drawn from 1,000 distinct
speakers discussing more than 250 topics. Each clip captures diverse recording conditions—varying in
camera distance, lighting, and background—reflecting natural, in-the-wild expression patterns. The
dataset provides continuous sentiment labels ranging from -3 (strongly negative) to +3 (strongly
positive), enabling fine-grained analysis of sentiment intensity. For CMU-MOSEI, we utilize the
provided pre-extracted features:
Visual features: OpenFace2 (713-dimensional) - facial action units, pose, gaze, and appearance
features
Audio features: COVAREP (74-dimensional) - prosodic, spectral, and cepstral features
Text features: GloVe embeddings (300-dimensional) - word-level semantic representations
We employ a standard 70/15/15 train/validation/test partition for CMU-MOSEI. These pre-extracted
features serve as the training data for our base multimodal fusion model and establish the target feature
space for our feature adaptation framework.
CMU-MOSI (Multimodal Opinion-Level Sentiment Intensity) [ 13] is a widely used multimodal dataset
for sentiment analysis, consisting of 2,199 video segments from 93 YouTube movie review videos. Each
segment is annotated for sentiment intensity, subjectivity, and various audio and visual features. Similar
to CMU-MOSEI, CMU-MOSI contains continuous sentiment labels from -3 to +3 and exhibits diverse
recording conditions typical of in-the-wild videos. Unlike CMU-MOSEI, CMU-MOSI requires real-time
feature extraction from raw video, audio, and text data. We extract features using:
Visual: MediaPipe FaceMesh (real-time processing)
Audio: Librosa (real-time audio analysis)
Text: BERT (contextual text embeddings)
4

CMU-MOSI serves as the target domain for our transfer learning evaluation. We apply the same
70/15/15 train/validation/test partition to CMU-MOSI for consistency.
To bridge the feature space gap between pre-extracted (MOSEI) and real-time (MOSI) features, we
employ neural feature adapter networks. Specifically, we train three separate two-layer feedforward
networks:
Visual Adapter: Maps FaceMesh features (65 dimensions) to OpenFace2-compatible representations
(713 dimensions)
Audio Adapter: Maps Librosa features (74 dimensions) to COVAREP-compatible features (74
dimensions)
Text Adapter: Maps BERT embeddings (768 dimensions) to GloVe-compatible vectors (300 dimen-
sions)
Each adapter is trained to minimize mean squared error between adapted MOSI features and randomly
sampled MOSEI target features, learning to align feature distributions across domains. The adapters
enable the pre-trained MOSEI model to process real-time extracted features without requiring retraining
of the base model, representing a form of feature space adaptation that preserves sentiment-relevant
information while transforming dimensions and aligning distributions.
3.2 Audio Pipeline
To represent the vocal dimension of emotion, we extract acoustic prosody features using Librosa [ 8], a
Python library for audio and music signal processing that provides standardized tools for converting
raw speech waveforms into quantitative descriptors of prosody and timbre.
Each audio track is processed using Librosa’s default parameters optimized for spectral analysis. We
load audio at a sampling rate of 22.05 kHz with a maximum duration of 3.0 seconds to balance
computational efficiency with temporal coverage. Librosa uses default Short-Time Fourier Transform
(STFT) parameters:
Frame size (n_fft): 2048 samples (93 ms at 22.05 kHz)
Hop length: 512 samples (23 ms at 22.05 kHz)
Window: Hann window with 75
Within each 3-second audio segment, we compute frame-level acoustic descriptors and then temporally
average across all frames to obtain a single vector representation per audio sample. Specifically, we
extract the following features:
Mel-frequency cepstral coefficients (MFCCs): 13 coefficients capturing spectral envelope charac-
teristics that reflect vocal tract shape
Chroma features: 12 coefficients representing pitch class distribution, which implicitly capture
fundamental frequency patterns
Spectral centroid: 1 coefficient indicating the "brightness" or spectral center of mass
Spectral rolloff: 1 coefficient marking the frequency below which a specified percentage of spectral
energy is contained
Zero-crossing rate (ZCR): 1 coefficient indicating voice quality and voicing characteristics
Tempo: 1 coefficient capturing rhythmic characteristics
This yields 29 frame-level features per frame. Temporal averaging (mean) is applied across all frames
within the 3-second segment, reducing the representation from [29 features×multiple frames] to a
fixed-size vector of 29 features per audio sample.
To ensure compatibility with the CMU-MOSEI dataset’s COVAREP audio feature format (74 dimen-
sions), the feature vector is zero-padded to 74 dimensions.
5

Thesefeaturescapturehowspeechenergy, pitchvariation, andspectralshapefluctuatewithemotion. For
example, higher pitch (reflected in chroma distributions) and spectral centroid values often correspond
to excitement or anger, while lower, flatter contours indicate sadness or fatigue.
The 74-dimensional audio features are passed through the audio encoder, which consists of two linear
transformations:
Layer 1: 74→192 dimensions (with BatchNorm1d, ReLU activation, Dropout 0.7)
Layer 2: 192→192 dimensions (with BatchNorm1d, ReLU activation, Dropout 0.7)
Output Layer: 192→96 dimensions (with BatchNorm1d)
Theencoderproducesa96-dimensionalrepresentation(embed dim= 96) thatisusedincross−modalattentionand fusion.
—
3.3 Visual Pipeline
For visual feature extraction, we process video frames using MediaPipe FaceMesh [ 5] to extract 468 3D
facial landmarks per frame. To handle variable-length videos efficiently, we process frames sequentially
from the start of each video, processing up to the first 100 frames per video (or all frames if the video
contains fewer than 100 frames). This corresponds to approximately 3.3 seconds at 30 fps, though
the implementation is frame-based rather than time-based. Frame-level features are extracted at the
video’s native frame rate, typically 30 fps.
For each frame, we derive 65-dimensional emotion-focused features through geometric computations,
with the face first normalized by face width (computed as the Euclidean distance between landmarks at
indices 0 and 16) per frame to handle scale variation across videos and frames. We extract 12 explicitly
defined emotion-relevant features:
Mouth characteristics (5 features): mouth width (Euclidean distance between landmarks 61 and
291), mouth height (distance between landmarks 13 and 14), left and right mouth corner Y-coordinates
(landmarks 61 and 291), and mouth corner angle computed using arctangent
Eye features (3 features): left eye width (distance between landmarks 33 and 133), right eye width
(distance between landmarks 362 and 263), and inter-eye distance (distance between landmarks 33 and
263)
Eyebrow features (2 features): average heights for left eyebrow (using landmarks 21, 55, and 107)
and right eyebrow (using landmarks 251, 285, and 336) Symmetry metrics (2 features): eye symmetry
(normalized absolute difference between left and right eye widths) and mouth symmetry (absolute
difference between left and right corner Y coordinates)
The remaining 53 features are derived from normalized landmark magnitudes: we compute the
Euclidean norm (L2 norm) of normalized landmark coordinates at indices 12 through 64 (inclusive),
providing additional geometric information about facial structure relative to face scale. This feature
extraction approach provides a more compact and semantically meaningful representation than using
raw landmark coordinates directly, focusing computational resources on emotion-relevant facial regions
while maintaining sufficient geometric information for sentiment analysis. This yields exactly 65 features
per frame: [1 frame×65 features]. Temporal Aggregation: Features are extracted at the frame level
(one 65-dimensional vector per frame) and then temporally averaged across all processed frames (up
to 100, or all frames for shorter videos) to obtain a single video-level representation: [1 video×65
features]. This temporal averaging strategy provides a stable representation that captures overall facial
expression patterns while remaining computationally efficient and compatible with the fixed-size input
requirements of the encoder architecture.
The video-level feature vector [1 video×65 features] is passed through the visual encoder, which
consists of two linear transformations (65→192→96 dimensions) with batch normalization, ReLU
activation, and dropout (0.7). We employ dropout of 0.7 in all encoder and fusion layers to provide
strong regularization and prevent overfitting, which is particularly important for multimodal models
with limited training data. The encoder produces a 96-dimensional representation (embed_dim = 96),
matching the text encoder output dimension. This encoded representation is then used in cross-modal
6

attention and fusion. Since the visual modality produces a single vector per video (not 30 frames like
audio), it is broadcast or replicated as needed during cross-modal fusion operations.
3.4 Text pipeline
To represent the linguistic dimension of emotion, we extract contextualized semantic features from
transcripts using BERT-base-uncased [ 11], a bidirectional transformer model pretrained on large-scale
English corpora. BERT-base-uncased consists of:
- 12 transformer layers with 768-dimensional hidden states
- 12 attention heads per layer
-Vocabulary size: 30,522 tokens
-Maximum sequence length: 512 tokens
-Model size: 110M parameters
Transcripts are processed through the BERT-base-uncased tokenizer, which splits text into subword
tokens using WordPiece tokenization. Special tokens are automatically added:
-[CLS]: Added at the start of each sequence (token ID=101)
-[SEP]: Added between sentences or at sequence end (token ID=102)
-[PAD]: Added for sequences shorter than 512 tokens (token ID=0)
We set a maximum sequence length of 512 tokens, with shorter sequences padded using [PAD] tokens
and longer sequences truncated from the right. The tokenizer produces token ID sequences and an
attention mask, where 1 indicates real tokens and 0 indicates padding tokens.
We use BERT as a frozen encoder (model.eval() with requires_grad=False for all parameters), leveraging
pretrained representations without task-specific fine-tuning to maintain generalization and reduce
overfitting. The model processes tokenized input to produce hidden states of shape [batch_size,
sequence_length, 768], where each token is represented by a 768-dimensional vector.
To obtain a single fixed-size representation per transcript, we apply mean pooling over the sequence
dimension, computing the average of all token embeddings. This yields a 768-dimensional vector
capturing the overall semantic content of the transcript.
For transfer learning compatibility with CMU-MOSEI’s GloVe embeddings (300 dimensions), the
768-dimensional BERT embeddings are passed through a neural feature adapter network. The adapter
is a two-layer feedforward network with the architecture:
Input Layer: Linear(768→384)→BatchNorm1d→ReLU→Dropout(0.3)
Hidden Layer: Linear(384→384)→BatchNorm1d→ReLU→Dropout(0.3)
Output Layer: Linear(384→300)
The adapter is trained to minimize mean squared error between adapted BERT embeddings and
randomly sampled GloVe target features from CMU-MOSEI, learning to map BERT embeddings to
GloVe-compatible representations.
The adapted text features (300 dimensions) are then processed through a text encoder consisting of
two linear transformations:
Layer 1: 300→192 dimensions (with BatchNorm1d, ReLU activation, Dropout 0.7)
Layer 2: 192→192 dimensions (with BatchNorm1d, ReLU activation, Dropout 0.7)
Output Layer: 192→96 dimensions (with BatchNorm1d)
This encoder produces the final text representation of dimension 96 (embed_dim), matching the visual
and audio encoder output dimensions. The encoder learns to project text features into a shared semantic
space suitable for cross-modal attention and fusion.
7

3.5 Temporal Alignment
In order for all three modalities to work together, they must match up in time-series data to find
correspondence. Without it, the correlation can be spurious. After feature extraction, the three
modalities have a different number of frames. Each modality can’t concatenate or compare to a single
unified representation because of the unequal frames. In order for each modality to match up, a function
needs to be applied. For my visual modality, it is set on 30 frames, so it does not change, but the other
modalities have too many frames incorporated.
For the Audio modality to be matched up with the visual modality, Adaptive Average Pooling 1D
is applied to the modality. For adaptive average pooling 1D, I specify what output I want to reach,
and the pooling adapts to that specific output. Internally, the adaptive average pooling first splits the
number of frames into a certain number of bins. Since some bins can’t be fractional, some bins get a
little bit more than others, depending on the number of frames.
Once the adaptive average pooling is applied, a binning pattern is necessary to ensure proper consistency
across frames. The binning pattern ensures fairness across frames. With the Average Pooling 1D
applied, it does split the frames into certain bins, but all those bins are spread out unevenly. To ensure
that the pattern has fairness, the binning patterns make sure that all the frames are spread out evenly
so each section gets equal temporal resolutions.
For the text modality, I utilized linear interpolation. Linear interpolation is a Machine Learning method
used to fill gaps in data, create new data points, and generate smooth curves. For my text modality,
the issue is that there are a few tokens, so more needs to be generated to be fit with the visual modality.
In order to mitigate this issue, new frames are created by blending between existing tokens. The first
step to text interpolation is mapping tokens to positions. Once the tokens are mapped, the space gets
filled in to compensate for the missing frames.
3.6 Transfer Learning
The transfer learning framework operates in three sequential phases:
3.6.1 Training on CMU-MOSEI
In the first phase, we train the base multimodal fusion model on CMU-MOSEI using pre-extracted
features:
Visual: OpenFace2 features (713-dimensional)Audio: COVAREP features (74-dimensional)Text:
GloVe embeddings (300-dimensional)
The model is trained for 100 epochs with early stopping (patience=25) based on validation correlation.
The training uses:
Batch size: 32
Learning rate: 0.0008
Weight decay: 0.04
Optimizer: Adam
Scheduler: ReduceLROnPlateau (factor=0.7, patience=7, mode=’max’)
The best model (based on validation correlation) is saved for use in subsequent phases.
3.6.2 Training Feature Adapters
In the second phase, we train three feature adapter networks to map real-time extracted features
(from CMU-MOSI) to the pre-extracted feature space (from CMU-MOSEI). Each adapter is trained
independently using mean squared error loss:
Visual Adapter: Maps FaceMesh features (65-dim)→OpenFace2 features (713-dim) - Architecture:
Linear(65→512)→BatchNorm→ReLU→Dropout(0.3)→Linear(512→512)→BatchNorm→
ReLU→Dropout(0.3)→Linear(512→713)
8

Audio Adapter: Maps Librosa features (74-dim)→COVAREP features (74-dim) - Architecture:
Linear(74→256)→BatchNorm→ReLU→Dropout(0.3)→Linear(256→256)→BatchNorm→
ReLU→Dropout(0.3)→Linear(256→74)
Text Adapter: Maps BERT embeddings (768-dim)→GloVe embeddings (300-dim) - Architecture:
Linear(768→384)→BatchNorm→ReLU→Dropout(0.3)→Linear(384→384)→BatchNorm→
ReLU→Dropout(0.3)→Linear(384→300)
Training parameters:
- Batch size: 16
- Learning rate: 0.001
- Optimizer: Adam
- Epochs: 30
- Loss: Mean Squared Error (MSE)
During training, target features are randomly sampled from CMU-MOSEI data (1000 samples) and
paired with extracted CMU-MOSI features to minimize the reconstruction error.
3.6.3 Testing on CMU-MOSI
In the third phase, we evaluate the transfer learning framework by:
1. Extracting features from CMU-MOSI using real-time extractors (FaceMesh, Librosa, BERT)
2. Adapting features using the trained adapter networks
3. Processing adapted features through the pre-trained MOSEI model
4. Evaluating sentiment prediction performance on CMU-MOSI test set
This approach enables cross-dataset generalization while preserving the benefits of training on large-scale
pre-extracted features.
3.7 Multitask Loss Weighting
A critical challenge in multi-task learning is that different loss functions [ 4] operate at different scales.
In our case, MSE values typically range from 0 to 1 for normalized sentiment scores, while cross-entropy
loss can span from 0 to 10 or higher depending on prediction confidence. Naively summing these
losses would result in the classification objective dominating the optimization process due to its larger
magnitude, effectively causing the model to neglect regression performance. This scale imbalance
manifests in gradient magnitudes: cross-entropy gradients can be an order of magnitude larger than
MSE gradients, causing parameter updates to be primarily driven by classification performance.
To address this imbalance, we employ a weighted combination of the two losses:
Ltotal=α·L MSE +β·L CE
where αand βare weighting coefficients that balance the contribution of each task. We determined
optimal coefficients through systematic grid search on the validation set, evaluating combinations
where α–> 1, 5, 10, 20 and β= 1.0, selecting values that optimize validation performance across both
regression (MAE, correlation) and classification (accuracy) metrics. Based on this search, we set α
= 10.0 for the regression loss and β= 1.0 for the classification loss. This weighting scheme balances
the relative contribution of both tasks, ensuring that both regression and classification objectives
meaningfully influence parameter updates throughout training. The selected ratio of 10:1 reflects the
approximate ratio of gradient magnitudes observed during initial training, though we acknowledge that
optimal weighting may vary across architectures and datasets.
9

4 Results
4.1 Validation
r=P(xi−¯x)(y i−¯y)pP(xi−¯x)2(yi−¯y)2
To validate that the detection of emotion through the three modalities is viable, we use multiple
evaluation metrics: Mean Absolute Error (MAE), Pearson Correlation Coefficient, and a combined loss
function that guides model training. The Pearson Correlation Coefficient measures the strength and
direction of the linear relationship between predictions and ground truth labels, ranging from -1 to +1.
A positive correlation means predictions increase with targets (positive relationship), while a negative
correlation means predictions decrease as targets increase (inverse relationship). In sentiment analysis,
we compare model predictions with human-annotated sentiment labels across the test set using the
formula:
MAE=1
nnX
i=1|yi−ˆy|
The Mean Absolute Error (MAE) measures the average magnitude of prediction errors in the same
units as the target variable, indicating how far predictions deviate from true values. MAE is preferred
because it is interpretable, treats errors linearly (outliers do not dominate), and provides a balanced
view of typical performance. MAE complements correlation because correlation measures rank/order
similarity, while MAE measures actual error magnitude. Together, they provide a clear picture of model
performance.
To guide learning, we use a combined loss function that jointly optimizes prediction accuracy and rank
consistency. The loss function combines Mean Squared Error (MSE), Mean Absolute Error (MAE), and
Pearson correlation coefficient to ensure both accurate values and correct relative ordering of sentiment
intensities. Specifically, we average MSE and MAE for balanced absolute error signals, then combine
this with correlation loss to emphasize rank consistency:
LCombined =α·L MSE +β·L correlation
where α=0.3 and β=0.7, prioritizing correlation optimization while maintaining reasonable absolute
accuracy. The correlation loss is computed as (1-r) to the power of 2 where r is the Pearson correlation
coefficient, providing a stronger gradient signal for correlation improvements compared to linear
correlation loss. This weighting ensures that both absolute accuracy (through MSE and MAE) and
relative ordering (through correlation) meaningfully influence parameter updates throughout training,
enabling the model to capture both precise sentiment magnitudes and correct sentiment rankings.
4.2 Architecture
The proposed multimodal sentiment analysis framework processes three input modalities through
a hierarchical architecture, transforming extracted features into unified sentiment predictions. The
architecture consists of four main components: modality encoders, cross-modal attention, fusion layers,
and prediction head.
4.2.1 Modality Encoders
Each modality (visual, audio, text) is processed through its respective encoder to transform input
features into a unified 96-dimensional embedding space (embed_dim = 96). All encoders share the
same architecture pattern:
Input Layer: Input_dim→192 dimensions (Linear, BatchNorm1d, ReLU, Dropout 0.7)
Hidden Layer: 192→192 dimensions (Linear, BatchNorm1d, ReLU, Dropout 0.7)
Output Layer: 192→96 dimensions (Linear, BatchNorm1d)
10

The encoders transform:
Visual: 65-dimensional FaceMesh features→96-dimensional embeddings
Audio: 74-dimensional Librosa features→96-dimensional embeddings
Text: 300-dimensional GloVe-compatible features→96-dimensional embeddings
4.2.2 Cross-Modal Attention
Cross-modal attention mechanisms enable each modality to dynamically attend to relevant information
from the other modalities. For three modalities (Visual, Audio, Text), we compute bidirectional
attention between all pairs using Multi-head-Attention with:
Number of heads: 4
Embedding dimension: 96 (embed_dim)
Dropout: 0.8 (min(dropout + 0.1, 0.8) where dropout=0.7)
The encoded features are stacked along a new dimension: [v_enc, a_enc, t_enc]→[batch_size, 3, 96].
Cross-attention is applied as:
Attention(Q, K, V) =softmax(QKT
√dk)V
where queries (Q), keys (K), and values (V) are derived from the stacked feature representations. This
produces attended features that capture inter-modal relationships.
4.2.3 Fusion Layers
Following cross-modal attention, the three encoded modalities are concatenated along the feature
dimension to form a unified representation:
Concatenated Features: [batch_size, 288] (96×3 modalities)
This concatenated representation is passed through a three-layer fusion network:
Layer 1: Linear(288→192)→BatchNorm1d→ReLU→Dropout(0.7)
Layer 2: Linear(192→96)→BatchNorm1d→ReLU→Dropout(0.7)
Layer 3: Linear(96→1)
The fusion network compresses the multimodal representation while preserving sentiment-relevant
information from all three modalities.
4.2.4 Prediction
The final layer produces a single scalar output representing the predicted sentiment score in the range
[-3, +3]. The output is used directly for regression-based sentiment prediction without additional
activation functions, allowing the model to learn the full range of sentiment intensity.
11

Figure 1: Architectural model of the multimodal system.
4.3 Data
Table 1: The Average Train Loss, Train MAE, Train Corr, Val Loss, Val MAE, and VAL Corr for each
modality combination. These are the results from the testing of pre-extracted data using COVAREP,
GLoVe, and text. These results serve to determine which modality combination has the best results.
Modality Train Loss Train MAE Train Corr Val Loss Val MAE Val Corr
Audio 0.5678 0.5574 0.2604 0.5423 0.5656 0.2703
Visual 0.5632 0.5642 0.2625 0.4797 0.5659 0.3265
Text 0.1093 0.3401 0.7932 0.2142 0.4440 0.6526
Audio + Visual 0.4680 0.5398 0.3494 0.4233 0.5395 0.3835
Audio + Text 0.1064 0.3362 0.8024 0.2138 0.4330 0.6705
Text + Visual 0.1086 0.3379 0.7951 0.2160 0.4398 0.6686
Text + Visual + Audio 0.1075 0.3358 0.7979 0.2197 0.4462 0.6622
As I received the trained data from CMU-MOSEI, I was able to see the efficiency of what each modality
brings to the dataset. From analyzing, having a multimodal system is ultimately more efficient than
a single modality, with the exception of text being better than Audio + Visual. Other than that,
multimodal systems surpass other systems with a training loss that is 5 times less, and a correlation
that is nearly 3 times more.
12

Table 2: The Average Train Loss, Train MAE, Train Corr, Val Loss, Val MAE, and VAL Corr for each
modality combination. These are the results from the testing of pre-extracted data using COVAREP,
GLoVe, and text. This results serve as which modality combination has the best results.
Modality Train Loss Train MAE Train Corr Val Loss Val MAE Val Corr
Audio 0.5678 0.5574 0.2604 0.5423 0.5656 0.2703
Visual 0.5632 0.5642 0.2625 0.4797 0.5659 0.3265
Text 0.1093 0.3401 0.7932 0.2142 0.4440 0.6526
Audio + Visual 0.4680 0.5398 0.3494 0.4233 0.5395 0.3835
Audio + Text 0.1064 0.3362 0.8024 0.2138 0.4330 0.6705
Text + Visual 0.1086 0.3379 0.7951 0.2160 0.4398 0.6686
Text + Visual + Audio 0.1075 0.3358 0.7979 0.2197 0.4462 0.6622
5 acknowledgments
References
[1]Boualeb, F., Pierson, E., Doudeau, N., Nineuil, C., Amad, A., and Daoudi, M. (2025). Measuring
anxiety levels with head motion patterns in severe depression population. In2025 IEEE 19th
International Conference on Automatic Face and Gesture Recognition (FG), pages 1–10. IEEE.
[2]Dibeklioğlu, H., Hammal, Z., and Cohn, J. F. (2017). Dynamic multimodal measurement of
depression severity using deep autoencoding.IEEE journal of biomedical and health informatics,
22(2):525–536.
[3]Gajjar, J. and Ranaware, K. (2025). Multimodal sentiment analysis on cmu-mosei dataset using
transformer-based models.arXiv preprint arXiv:2505.06110.
[4]Golnari, A. and Diba, M. (2024). Adaptive real-time multi-loss function optimization using
dynamic memory fusion framework: A case study on breast cancer segmentation.arXiv preprint
arXiv:2410.19745.
[5]Grishchenko, I., Ablavatski, A., Kartynnik, Y., Raveendran, K., and Grundmann, M. (2020).
Attention mesh: High-fidelity face mesh prediction in real-time.arXiv preprint arXiv:2006.10962.
[6]Hale III, W. W. and Aarts, E. (2023). Hidden markov model detection of interpersonal interaction
dynamics in predicting patient depression improvement in psychotherapy: Proof-of-concept study.
Journal of Affective Disorders Reports, 14:100635.
[7]Hall, N. T., Hallquist, M. N., Martin, E. A., Lian, W., Jonas, K. G., and Kotov, R. (2024). Automat-
ing the analysis of facial emotion expression dynamics: A computational framework and application
in psychotic disorders.Proceedings of the National Academy of Sciences, 121(14):e2313665121.
[8]McFee, B., Raffel, C., Liang, D., Ellis, D. P., McVicar, M., Battenberg, E., and Nieto, O. (2015).
librosa: Audio and music signal analysis in python.SciPy, 2015:18–24.
[9]Samareh, A., Jin, Y., Wang, Z., Chang, X., and Huang, S. (2018). Detect depression from
communication: How computer vision, signal processing, and sentiment analysis join forces.IISE
transactions on healthcare systems engineering, 8(3):196–208.
[10]Singh, A. and Kumar, D. (2022). Detection of stress, anxiety and depression (sad) in video
surveillance using resnet-101.Microprocessors and Microsystems, 95:104681.
[11]Song, X., Salcianu, A., Song, Y., Dopson, D., and Zhou, D. (2020). Fast wordpiece tokenization.
arXiv preprint arXiv:2012.15524.
[12]Xu, X., Zhang, X., and Zhang, Y. (2024). Faces of the mind: Unveiling mental health states
through facial expressions in 11,427 adolescents.arXiv preprint arXiv:2405.20072.
[13]Zadeh, A., Zellers, R., Pincus, E., and Morency, L.-P. (2016). Mosi: multimodal corpus of sentiment
intensity and subjectivity analysis in online opinion videos.arXiv preprint arXiv:1606.06259.
13

